# File: docker-compose.yml (Versi Final Gabungan Batch + Streaming Otomatis)
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment: { ZOOKEEPER_CLIENT_PORT: 2181, ZOOKEEPER_TICK_TIME: 2000 }

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    depends_on: [zookeeper]
    ports: ["9092:9092"]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  minio:
    image: minio/minio:latest
    container_name: minio
    ports: ["9000:9000", "9001:9001"]
    volumes: ["./data:/data"]
    environment: { MINIO_ROOT_USER: minioadmin, MINIO_ROOT_PASSWORD: minioadmin }
    command: server /data --console-address ":9001"

  # Layanan Streamlit yang menjadi PUSAT KENDALI UI
  streamlit_app:
    build:
      context: ./streamlit
      dockerfile: Dockerfile
    container_name: streamlit_app
    ports: ["8501:8501"]
    volumes: ["./streamlit:/app"]
    depends_on: [minio, kafka]
    restart: always

  # --- LAYANAN BACKGROUND & PEKERJA OTOMATIS ---

  # Layanan untuk menjalankan producer streaming secara terus-menerus
  kafka_producer:
    build:
      context: ./producer_app
    container_name: kafka_producer
    depends_on:
      - kafka
    # Volume ini yang akan 'memasukkan' data ke dalam kontainer saat berjalan
    volumes:
      - ./data/final_movies_dataset.csv:/app/final_movies_dataset.csv
    restart: on-failure

  # Layanan yang HANYA berjalan sekali untuk ingesti & training model batch
  # Layanan ini tidak diperlukan jika Anda hanya ingin fokus pada streaming ETL
  # Namun kita sertakan agar fitur "Rekomendasi Film" berfungsi.
  batch_setup_job:
    build: { context: ./spark } # Menggunakan image Spark
    container_name: batch_setup_job
    volumes:
      - ./spark:/opt/bitnami/spark/app
      - ./ingestion:/ingestion
      - ./data:/data
    depends_on: [minio, spark-master]
    command: python /opt/bitnami/spark/app/run_setup_and_batch.py # Skrip orkestrasi kita
    restart: on-failure

  # Layanan Spark Master & Worker (hanya untuk job batch)
  spark-master:
    build: { context: ./spark }
    container_name: spark-master
    ports: ["8080:8080", "4040:4040"]
    volumes: ["./spark:/opt/bitnami/spark/app"]

  spark-worker:
    build: { context: ./spark }
    container_name: spark-worker
    depends_on: [spark-master]
    volumes: ["./spark:/opt/bitnami/spark/app"]
    environment: { SPARK_MODE: worker, SPARK_MASTER_URL: spark://spark-master:7077 }